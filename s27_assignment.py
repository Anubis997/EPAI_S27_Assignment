# -*- coding: utf-8 -*-
"""S27_Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U-0eEpR7ux2WlFg4rmfzKdzu42_-n1Hk
"""

import torch
import torch.nn as nn
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from tqdm import tqdm, trange
import numpy as np

# Check for GPU availability
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

class CustomCNN:
    def __init__(self):
        # Initialize weights on GPU
        self.conv1_weights = (torch.randn(16, 1, 3, 3) * 0.01).to(device)
        self.conv1_bias = torch.zeros(16).to(device)
        self.conv2_weights = (torch.randn(32, 16, 3, 3) * 0.01).to(device)
        self.conv2_bias = torch.zeros(32).to(device)
        self.fc1_weights = (torch.randn(32 * 5 * 5, 128) * 0.01).to(device)
        self.fc1_bias = torch.zeros(128).to(device)
        self.fc2_weights = (torch.randn(128, 10) * 0.01).to(device)
        self.fc2_bias = torch.zeros(10).to(device)

        # Cache for storing intermediate values and shapes
        self.cache = {}

    def custom_conv2d(self, x, weights, bias, layer_name='conv'):
        batch_size, in_channels, height, width = x.shape
        out_channels, _, kernel_size, _ = weights.shape

        out_height = height - kernel_size + 1
        out_width = width - kernel_size + 1
        output = torch.zeros(batch_size, out_channels, out_height, out_width, device=device)

        # Store input windows for backprop
        windows = torch.zeros(batch_size, out_channels, out_height, out_width,
                            in_channels, kernel_size, kernel_size, device=device)

        for b in range(batch_size):
            for oc in range(out_channels):
                for h in range(out_height):
                    for w in range(out_width):
                        window = x[b, :, h:h+kernel_size, w:w+kernel_size]
                        windows[b, oc, h, w] = window
                        output[b, oc, h, w] = torch.sum(window * weights[oc]) + bias[oc]

        self.cache[f'{layer_name}_x'] = x
        self.cache[f'{layer_name}_windows'] = windows

        return output

    def conv2d_backward(self, grad_output, layer_name='conv'):
        x = self.cache[f'{layer_name}_x']
        windows = self.cache[f'{layer_name}_windows']
        weights = getattr(self, f'{layer_name}_weights')

        batch_size, out_channels, out_height, out_width = grad_output.shape
        _, in_channels, kernel_size, _ = weights.shape

        grad_input = torch.zeros_like(x, device=device)
        grad_weights = torch.zeros_like(weights, device=device)
        grad_bias = torch.zeros(out_channels, device=device)

        for b in range(batch_size):
            for oc in range(out_channels):
                for h in range(out_height):
                    for w in range(out_width):
                        window_grad = grad_output[b, oc, h, w] * weights[oc]
                        grad_input[b, :, h:h+kernel_size, w:w+kernel_size] += window_grad

                        window = windows[b, oc, h, w]
                        grad_weights[oc] += grad_output[b, oc, h, w] * window

                        grad_bias[oc] += grad_output[b, oc, h, w]

        return grad_input, grad_weights, grad_bias

    def custom_max_pool2d(self, x, kernel_size=2, stride=2, layer_name='pool'):
        batch_size, channels, height, width = x.shape
        out_height = (height - kernel_size) // stride + 1
        out_width = (width - kernel_size) // stride + 1
        output = torch.zeros(batch_size, channels, out_height, out_width, device=device)

        indices = torch.zeros_like(output, dtype=torch.int64, device=device)

        for b in range(batch_size):
            for c in range(channels):
                for h in range(out_height):
                    for w in range(out_width):
                        h_start = h * stride
                        w_start = w * stride
                        window = x[b, c, h_start:h_start+kernel_size, w_start:w_start+kernel_size]
                        max_val, max_idx = torch.max(window.flatten(), 0)
                        output[b, c, h, w] = max_val
                        indices[b, c, h, w] = max_idx

        self.cache[f'{layer_name}_x'] = x
        self.cache[f'{layer_name}_indices'] = indices

        return output

    def maxpool_backward(self, grad_output, layer_name='pool'):
        x = self.cache[f'{layer_name}_x']
        indices = self.cache[f'{layer_name}_indices']

        batch_size, channels, out_height, out_width = grad_output.shape
        grad_input = torch.zeros_like(x, device=device)

        kernel_size = 2
        stride = 2

        for b in range(batch_size):
            for c in range(channels):
                for h in range(out_height):
                    for w in range(out_width):
                        h_start = h * stride
                        w_start = w * stride
                        idx = indices[b, c, h, w]
                        idx_h = (idx // kernel_size).long()
                        idx_w = (idx % kernel_size).long()
                        grad_input[b, c, h_start + idx_h, w_start + idx_w] = grad_output[b, c, h, w]

        return grad_input

    def relu(self, x, layer_name='relu'):
        output = torch.maximum(torch.tensor(0.0, device=device), x)
        self.cache[f'{layer_name}_mask'] = (x > 0).float()
        return output

    def relu_backward(self, grad_output, layer_name='relu'):
        mask = self.cache[f'{layer_name}_mask']
        return grad_output * mask

    def forward(self, x):
        # Move input to GPU if it's not already there
        x = x.to(device)

        # First conv layer
        conv1 = self.custom_conv2d(x, self.conv1_weights, self.conv1_bias, 'conv1')
        relu1 = self.relu(conv1, 'relu1')
        pool1 = self.custom_max_pool2d(relu1, layer_name='pool1')

        # Second conv layer
        conv2 = self.custom_conv2d(pool1, self.conv2_weights, self.conv2_bias, 'conv2')
        relu2 = self.relu(conv2, 'relu2')
        pool2 = self.custom_max_pool2d(relu2, layer_name='pool2')

        # Flatten
        batch_size = x.shape[0]
        flattened = pool2.reshape(batch_size, -1)
        self.cache['flatten_shape'] = pool2.shape

        # Fully connected layers
        fc1 = torch.matmul(flattened, self.fc1_weights) + self.fc1_bias
        relu3 = self.relu(fc1, 'relu3')
        output = torch.matmul(relu3, self.fc2_weights) + self.fc2_bias

        return output

    def backward(self, grad_output):
        batch_size = grad_output.shape[0]

        # FC2 backward
        grad_relu3 = torch.matmul(grad_output, self.fc2_weights.t())
        grad_fc2_weights = torch.matmul(self.cache['relu3_mask'].t(), grad_output)
        grad_fc2_bias = grad_output.sum(dim=0)

        # ReLU3 backward
        grad_fc1 = self.relu_backward(grad_relu3, 'relu3')

        # FC1 backward
        flatten_shape = self.cache['flatten_shape']
        grad_flatten = torch.matmul(grad_fc1, self.fc1_weights.t())
        grad_fc1_weights = torch.matmul(grad_flatten.reshape(batch_size, -1).t(), grad_fc1)
        grad_fc1_bias = grad_fc1.sum(dim=0)

        # Reshape back to conv shape
        grad_pool2 = grad_flatten.reshape(flatten_shape)

        # Pool2 backward
        grad_relu2 = self.maxpool_backward(grad_pool2, 'pool2')

        # ReLU2 backward
        grad_conv2 = self.relu_backward(grad_relu2, 'relu2')

        # Conv2 backward
        grad_pool1, grad_conv2_weights, grad_conv2_bias = self.conv2d_backward(grad_conv2, 'conv2')

        # Pool1 backward
        grad_relu1 = self.maxpool_backward(grad_pool1, 'pool1')

        # ReLU1 backward
        grad_conv1 = self.relu_backward(grad_relu1, 'relu1')

        # Conv1 backward
        grad_input, grad_conv1_weights, grad_conv1_bias = self.conv2d_backward(grad_conv1, 'conv1')

        # Update weights
        learning_rate = 0.001
        self.conv1_weights -= learning_rate * grad_conv1_weights
        self.conv1_bias -= learning_rate * grad_conv1_bias
        self.conv2_weights -= learning_rate * grad_conv2_weights
        self.conv2_bias -= learning_rate * grad_conv2_bias
        self.fc1_weights -= learning_rate * grad_fc1_weights
        self.fc1_bias -= learning_rate * grad_fc1_bias
        self.fc2_weights -= learning_rate * grad_fc2_weights
        self.fc2_bias -= learning_rate * grad_fc2_bias

    def train_step(self, x, y):
        # Move input and target to GPU
        x = x.to(device)
        y = y.to(device)

        # Forward pass
        outputs = self.forward(x)

        # Compute cross-entropy loss
        exp_outputs = torch.exp(outputs)
        probabilities = exp_outputs / exp_outputs.sum(dim=1, keepdim=True)
        loss = -torch.mean(torch.log(probabilities[range(len(y)), y] + 1e-10))

        # Compute gradients
        grad_outputs = probabilities.clone()
        grad_outputs[range(len(y)), y] -= 1
        grad_outputs /= len(y)

        # Backward pass
        self.backward(grad_outputs)

        return loss.item()

def load_mnist_data(batch_size=64):  # Increased default batch size for GPU
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])

    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST('./data', train=False, transform=transform)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)  # Added pin_memory=True for faster GPU transfer
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)

    return train_loader, test_loader

def train_model(model, train_loader, num_epochs=5):
    epoch_pbar = trange(num_epochs, desc='Epochs')

    for epoch in epoch_pbar:
        total_loss = 0
        batch_pbar = tqdm(train_loader, desc=f'Epoch {epoch}', leave=False)

        for batch_idx, (data, target) in enumerate(batch_pbar):
            loss = model.train_step(data, target)
            total_loss += loss
            batch_pbar.set_postfix({'loss': f'{loss:.4f}'})

        avg_loss = total_loss / len(train_loader)
        epoch_pbar.set_postfix({'avg_loss': f'{avg_loss:.4f}'})
        batch_pbar.close()

def evaluate_model(model, test_loader):
    correct = 0
    total = 0

    eval_pbar = tqdm(test_loader, desc='Evaluating')

    with torch.no_grad():
        for data, target in eval_pbar:
            # Move data to GPU
            data, target = data.to(device), target.to(device)

            outputs = model.forward(data)
            _, predicted = torch.max(outputs.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

            current_acc = 100 * correct / total
            eval_pbar.set_postfix({'accuracy': f'{current_acc:.2f}%'})

    final_accuracy = 100 * correct / total
    print(f'\nFinal Test Accuracy: {final_accuracy:.2f}%')
    return final_accuracy

def main():
    # Set parameters
    batch_size = 32  # Increased for GPU
    num_epochs = 3

    # Load data
    print("Loading MNIST data...")
    train_loader, test_loader = load_mnist_data(batch_size)

    # Create and train model
    print("\nInitializing model...")
    model = CustomCNN()

    print("\nStarting training...")
    train_model(model, train_loader, num_epochs)

    print("\nEvaluating model...")
    accuracy = evaluate_model(model, test_loader)

if __name__ == "__main__":
    main()

